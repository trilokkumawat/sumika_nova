;!function(){try { var e="undefined"!=typeof globalThis?globalThis:"undefined"!=typeof global?global:"undefined"!=typeof window?window:"undefined"!=typeof self?self:{},n=(new e.Error).stack;n&&((e._debugIds|| (e._debugIds={}))[n]="93438e69-2627-0cd7-dc7b-7409c1eba476")}catch(e){}}();
(globalThis.TURBOPACK||(globalThis.TURBOPACK=[])).push(["object"==typeof document?document.currentScript:void 0,478124,n=>{"use strict";n.s(["default",0,"declare namespace Supabase {\n  export interface ModelOptions {\n    /**\n     * Pool embeddings by taking their mean. Applies only for `gte-small` model\n     */\n    mean_pool?: boolean\n\n    /**\n     * Normalize the embeddings result. Applies only for `gte-small` model\n     */\n    normalize?: boolean\n\n    /**\n     * Stream response from model. Applies only for LLMs like `mistral` (default: false)\n     */\n    stream?: boolean\n\n    /**\n     * Automatically abort the request to the model after specified time (in seconds). Applies only for LLMs like `mistral` (default: 60)\n     */\n    timeout?: number\n\n    /**\n     * Mode for the inference API host. (default: 'ollama')\n     */\n    mode?: 'ollama' | 'openaicompatible'\n    signal?: AbortSignal\n  }\n\n  export class Session {\n    /**\n     * Create a new model session using given model\n     */\n    constructor(model: string, sessionOptions?: unknown)\n\n    /**\n     * Execute the given prompt in model session\n     */\n    run(\n      prompt:\n        | string\n        | Omit<import('npm:openai@^4.52.5').OpenAI.Chat.ChatCompletionCreateParams, 'model' | 'stream'>,\n      modelOptions?: ModelOptions\n    ): unknown\n  }\n\n  /**\n   * Provides AI related APIs\n   */\n  export interface Ai {\n    readonly Session: typeof Session\n  }\n\n  /**\n   * Provides AI related APIs\n   */\n  export const ai: Ai\n}\n\ndeclare namespace EdgeRuntime {\n  export function waitUntil<T>(promise: Promise<T>): Promise<T>\n}\n"])}]);

//# debugId=93438e69-2627-0cd7-dc7b-7409c1eba476
//# sourceMappingURL=f24dd22a8821d0ee.js.map